{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN-BP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkCmHlxuEwtB",
        "outputId": "9ff34b23-a195-4244-ae69-f6e61c3ac001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: RuntimeWarning: overflow encountered in square\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I:0 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.102\n",
            "I:10 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:20 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:30 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:40 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:50 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:60 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:70 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:80 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:90 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:100 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:110 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:120 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:130 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:140 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:150 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:160 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:170 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:180 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:190 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:200 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:210 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:220 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:230 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:240 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:250 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:260 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:270 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:280 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:290 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:300 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:310 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:320 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:330 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097\n",
            "I:340 Test-Err:nan Test-Acc:0.098 Train-Err:nan Train-Acc:0.097"
          ]
        }
      ],
      "source": [
        "import sys, numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "images, labels = (x_train[0:1000].reshape(1000,28*28) / 255, y_train[0:1000])\n",
        "\n",
        "one_hot_labels = np.zeros((len(labels),10))\n",
        "for i,l in enumerate(labels):\n",
        "    one_hot_labels[i][l] = 1\n",
        "labels = one_hot_labels\n",
        "\n",
        "test_images = x_test.reshape(len(x_test),28*28) / 255\n",
        "test_labels = np.zeros((len(y_test),10))\n",
        "for i,l in enumerate(y_test):\n",
        "    test_labels[i][l] = 1\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "def relu(x):\n",
        "    return (x >= 0) * x # returns x if x > 0\n",
        "                        # returns 0 otherwise\n",
        "\n",
        "def relu2deriv(output):\n",
        "    return output >= 0 #returns 1 for input > 0\n",
        "\n",
        "\n",
        "\n",
        "#relu = lambda x:(x>=0) * x # returns x if x > 0, return 0 otherwise\n",
        "#relu2deriv = lambda x: x>=0 # returns 1 for input > 0, return 0 otherwise\n",
        "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
        "\n",
        "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
        "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
        "\n",
        "for j in range(iterations):\n",
        "    error, correct_cnt = (0.0, 0)\n",
        "    for i in range(int(len(images) / batch_size)):\n",
        "        batch_start, batch_end = ((i * batch_size),((i+1)*batch_size))\n",
        "\n",
        "        layer_0 = images[batch_start:batch_end]\n",
        "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
        "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
        "        layer_1 *= dropout_mask * 2\n",
        "        layer_2 = np.dot(layer_1,weights_1_2)\n",
        "\n",
        "        error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)\n",
        "        for k in range(batch_size):\n",
        "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
        "\n",
        "            layer_2_delta = (labels[batch_start:batch_end]-layer_2)/batch_size\n",
        "            layer_1_delta = layer_2_delta.dot(weights_1_2.T)* relu2deriv(layer_1)\n",
        "            layer_1_delta *= dropout_mask\n",
        "\n",
        "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "            \n",
        "    if(j%10 == 0):\n",
        "        test_error = 0.0\n",
        "        test_correct_cnt = 0\n",
        "\n",
        "        for i in range(len(test_images)):\n",
        "            layer_0 = test_images[i:i+1]\n",
        "            layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
        "            layer_2 = np.dot(layer_1, weights_1_2)\n",
        "\n",
        "            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
        "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
        "\n",
        "        sys.stdout.write(\"\\n\" + \\\n",
        "                         \"I:\" + str(j) + \\\n",
        "                         \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] +\\\n",
        "                         \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images)))+\\\n",
        "                         \" Train-Err:\" + str(error/ float(len(images)))[0:5] +\\\n",
        "                         \" Train-Acc:\" + str(correct_cnt/ float(len(images))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CejNEuYor03e",
        "outputId": "ecfb19f7-8cf0-4dc0-b655-53b332d70cbc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "9eGVN5lVr3Wc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L1sKl2yDwH_N",
        "outputId": "87a03d2e-b4ea-436f-c773-0109139a0191"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "j2Bi-6FOwJRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}